<style>

.reveal section p {
  color: black;
  font-size: .7em;
  font-weight: normal;
  font-family: 'Helvetica'; #this is the font/color of text in slides
}


.section .reveal .state-background {
    background: white;}
.section .reveal h1,
.section .reveal p {
    color: black;
    position: relative;
    font-family: 'Helvetica';
    font-weight: normal;
    top: 4%;}
   
 
 /* section titles */
.reveal h1 { 
  color: black;
  position: relative;
  font-weight: normal;
  font-family: 'Helvetica'; 
  top: 4%
}    

 
/* slide titles */
.reveal h3 { 
  color: black;
  font-weight: normal;
  font-family: 'Helvetica'; 
}    

.small-code pre code {
  font-size: 1.2em;
}

</style>



Basics of Text Analysis
========================================================
author: Chris Bail 
date: Duke University
autosize: true
transition: fade  
  website: https://www.chrisbail.net  
  Twitter: https://www.twitter.com/chris_bail  
  github: https://github.com/cbail  

========================================================

# **WRANGLING TEXT**

Character Encoding
========================================================

Character Encoding
========================================================
&nbsp;
<img src="character-viewer.png" height="400" />

UTF-8
========================================================
&nbsp;
<img src="utf8.jpg" height="400" />

Character Encoding Over Time
========================================================
&nbsp;
<img src="encoding_over_time.png" height="400" />


GREP
========================================================


GREP
========================================================

Globally search a regular expression and print

Regular Expression
========================================================

A pattern that describes a set of strings


It's GREP-tastic!
========================================================
&nbsp;

```{r}
duke_web_scrape<- "Duke Experts: A Trusted Source for Policymakers\n\n\t\t\t\t\t\t\t" 
```

grepl
========================================================
&nbsp;
```{r}
grepl("Experts", duke_web_scrape)
```


gsub
========================================================
&nbsp;
```{r}
gsub("\t", "", duke_web_scrape)
```

gsub (2 patterns)
========================================================
&nbsp;
```{r}
gsub("\t|\n", "", duke_web_scrape)
```

More GREP
========================================================
class: small-code
&nbsp;
```{r}
some_text<-c("Friends","don't","let","friends","make","wordclouds")

some_text[grep("^[F]", some_text)]
```

Regex Cheat Sheet
========================================================

<img src="regex.png" height="400" />

Link [here](http://www.rstudio.com/wp-content/uploads/2016/09/RegExCheatsheet.pdf)


Escaping Text
========================================================
&nbsp;
```{r, eval=FALSE}
text_chunk<-c("[This Professor is not so Great]")
gsub("[","", text_chunk)
```
&nbsp;  
<img src="regex_no.png" height="100" />

Escaping Text
========================================================
&nbsp;
```{r}
text_chunk<-c("[This Professor is not so Great]")
gsub('\\[|\\]',"", text_chunk)
```

========================================================

# **UNITS OF ANALYSIS**

Tokenization
========================================================

Tokenization
========================================================

```{r, eval=FALSE}
<sentence>
  
<word>Friends</word

<word>don't</word>

<word>let</word>

<word>friends</word>

<word>make</word>

<word>word</word>

<word>clouds</word>

</sentence>
```

N-grams
========================================================


N-grams
========================================================

<img src="8ARA1.png" height="400" />

========================================================

# **CREATING TEXT DATABASES**

What is a Corpus?
========================================================


Creating a Corpus
========================================================
class: small-code
&nbsp;  
```{r}
load(url("https://cbail.github.io/Trump_Tweets.Rdata"))

trumptweets$text[1]
```

Creating a Corpus
========================================================
class: small-code
&nbsp;
```{r, eval=FALSE}
install.packages("tm")
```

Creating a Corpus
========================================================
class: small-code
&nbsp;
```{r}
library(tm)
trump_corpus <- Corpus(VectorSource(as.vector(trumptweets$text))) 
trump_corpus
```

TidyText
========================================================




TidyText
========================================================
class: small-code
```{r}

library(tidytext)
library(dplyr)
tidy_trump_tweets<- trumptweets %>%
    select(created_at,text) %>%
    unnest_tokens("word", text)
head(tidy_trump_tweets)
```

TidyText is... tidy
========================================================
class: small-code

```{r}
tidy_trump_tweets %>%
  count(word) %>%
    arrange(desc(n))
```

Corpus vs. TidyText
========================================================




========================================================

# **TEXT PRE-PROCESSING**

Stopwords
========================================================


Stopwords
========================================================
class: small-code
In `tm`:

```{r}
trump_corpus <- tm_map(trump_corpus, removeWords, stopwords("english"))
```

Stopwords
========================================================
class: small-code

In `tidytext`:

```{r}
 data("stop_words")
    tidy_trump_tweets<-tidy_trump_tweets %>%
      anti_join(stop_words)
```

Inspect Top Words Again
========================================================
class: small-code

```{r}
tidy_trump_tweets %>%
  count(word) %>%
    arrange(desc(n))
```

Remove Punctuation
========================================================


Remove Punctuation
========================================================
class: small-code

In `tm`:

```{r}
trump_corpus <- tm_map(trump_corpus, content_transformer(removePunctuation))
```

(punctuation marks are removed automatically in `tidytext`)

Remove Numbers
========================================================
class: small-code

In `tm`:

```{r}
trump_corpus <- tm_map(trump_corpus, content_transformer(removeNumbers))
```

In `tidytext`:

```{r}
tidy_trump_tweets<-tidy_trump_tweets[-grep("\\b\\d+\\b", tidy_trump_tweets$word),]
```


Word Case
========================================================


Word Case
========================================================
class: small-code

In `tm`:

```{r}
trump_corpus <- tm_map(trump_corpus,  content_transformer(tolower)) 
```

Once again, `tidytext` does this for you.


Removing Whitespaces
========================================================


Removing Whitespaces
========================================================
class: small-code

In `tm`:

```{r}
trump_corpus <- tm_map(trump_corpus, content_transformer(stripWhitespace))
```

In `tidytext`:

```{r}
tidy_trump_tweets$word <- gsub("\\s+","",tidy_trump_tweets$word)
```

Stemming
========================================================

Stemming
========================================================
class: small-code

In `tm`:

```{r}
trump_corpus  <- tm_map(trump_corpus, content_transformer(stemDocument), language = "english")
```

In `tidytext`:
```{r}
library(SnowballC)
  tidy_trump_tweets<-tidy_trump_tweets %>%
      mutate_at("word", funs(wordStem((.), language="en")))
```

========================================================

# **THE DOCUMENT-TERM MATRIX**

The DTM
========================================================
class: small-code

In `tm`:

```{r}
trump_DTM <- 
  DocumentTermMatrix(trump_corpus, control = list(wordLengths=c(2, Inf)))

inspect(trump_DTM[1:5,3:8])

```

The DTM
========================================================
class: small-code

In `tidytext`:

```{r}
tidy_trump_DTM<-
  tidy_trump_tweets %>%
  count(created_at, word) %>%
  cast_dtm(created_at, word, n)
```


Other great resources
========================================================
&nbsp;  

Julia Silge's https://www.tidytextmining.com/  

Ken Benoit's https://quanteda.io/

Jurafsky and Martin's textbook: https://web.stanford.edu/~jurafsky/slp3/





